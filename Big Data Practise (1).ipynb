{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb19445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904d526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba9b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e581e",
   "metadata": {},
   "source": [
    "2 . \t\t  Process Titanic dataset using PySpark - SparkSQL libraries to answer questions given below \n",
    "\n",
    "i.\t\tLoad the  dataset into a Spark-DataFrame. \t2\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233805b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/03 10:38:26 WARN Utils: Your hostname, ikom-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp4s0)\n",
      "22/03/03 10:38:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ikom/nehal/pyspark/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/03 10:38:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/03 10:38:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdab7e014c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Demo').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b58373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading Data\n",
    "df_pyspark=spark.read.csv('/home/ikom/nehal/pyspark/titanic_new.csv',header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f69b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2cb834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=887, Survived=0, Pclass=2, Name='Montvila, Rev. Juozas', Sex='male', Age=27.0, SibSp=0, Parch=0, Ticket='211536', Fare=13.0, Cabin=None, Embarked='S'),\n",
       " Row(PassengerId=888, Survived=1, Pclass=1, Name='Graham, Miss. Margaret Edith', Sex='female', Age=19.0, SibSp=0, Parch=0, Ticket='112053', Fare=30.0, Cabin='B42', Embarked='S'),\n",
       " Row(PassengerId=889, Survived=0, Pclass=3, Name='\"Johnston, Miss. Catherine Helen \"\"Carrie\"\"\"', Sex='female', Age=None, SibSp=1, Parch=2, Ticket='W./C. 6607', Fare=23.45, Cabin=None, Embarked='S'),\n",
       " Row(PassengerId=890, Survived=1, Pclass=1, Name='Behr, Mr. Karl Howell', Sex='male', Age=26.0, SibSp=0, Parch=0, Ticket='111369', Fare=30.0, Cabin='C148', Embarked='C'),\n",
       " Row(PassengerId=891, Survived=0, Pclass=3, Name='Dooley, Mr. Patrick', Sex='male', Age=32.0, SibSp=0, Parch=0, Ticket='370376', Fare=7.75, Cabin=None, Embarked='Q')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4fbede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data types\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68ffe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1431ff4e",
   "metadata": {},
   "source": [
    "ii.\t\tHow many people travelled from Titanic (total count)?\t2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b3a06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4d1a0",
   "metadata": {},
   "source": [
    "iii.\t\tWhat was the average ticket fare?\t4\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d59a0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e49e372b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.2042079685746"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df_pyspark.select(mean(\"Fare\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83de6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Fare)=32.2042079685746)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(mean(\"Fare\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10b46c",
   "metadata": {},
   "source": [
    "iv.\t\t What was the average age of people travelled from titanic?\t6\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad55e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         avg(Age)|\n",
      "+-----------------+\n",
      "|29.69911764705882|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(mean(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb7293",
   "metadata": {},
   "source": [
    "v.\t\t What was the ratio of Lower class - Males to Upper class Females?\t8\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920a8b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a665764",
   "metadata": {},
   "source": [
    "vi.\t\t Which was the costliest cabin of titanic? What was its fair?\t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933446d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "#sqlContext.read.csv('titanic_new.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1850de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.createOrReplaceTempView('Titanic_table')\n",
    "sqlContext.sql(\"select * from Titanic_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdddce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bd4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2109db98",
   "metadata": {},
   "source": [
    "3\t\t\n",
    "Using SparkML libraries execute the steps, as questioned below, in order to build a PySpark classification ML-model on Titanic dataset\t\n",
    "\t\n",
    "i.\t\tFind null values count of every feature\t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd66465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId 0\n",
      "Survived 0\n",
      "Pclass 0\n",
      "Name 0\n",
      "Sex 0\n",
      "Age 177\n",
      "SibSp 0\n",
      "Parch 0\n",
      "Ticket 0\n",
      "Fare 0\n",
      "Cabin 687\n",
      "Embarked 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "for k in df_pyspark.columns:\n",
    "    print(k,df_pyspark.where(col(k).isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a95d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c843e",
   "metadata": {},
   "source": [
    "ii.\t\t For missing string feature add a new value as ‘missing’. For missing numeric values replace by the mode.\t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc80d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+-----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|Age_Imputed|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+-----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25| null|       S|       22.0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|       38.0|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Numerical Variables\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(\n",
    "    inputCol='Age',\n",
    "    outputCol='Age_Imputed',\n",
    ").setStrategy(\"mode\")\n",
    "\n",
    "df_imputed=imputer.fit(df_pyspark).transform(df_pyspark)\n",
    "df_imputed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0e34749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Categorical Variables\n",
    "df_imputed=df_imputed.na.fill({'Cabin':'Missing','Embarked':'Missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec9d0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed=df_imputed.drop('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d89157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId 0\n",
      "Survived 0\n",
      "Pclass 0\n",
      "Name 0\n",
      "Sex 0\n",
      "SibSp 0\n",
      "Parch 0\n",
      "Ticket 0\n",
      "Fare 0\n",
      "Cabin 0\n",
      "Embarked 0\n",
      "Age_Imputed 0\n"
     ]
    }
   ],
   "source": [
    "for k in df_imputed.columns:\n",
    "    print(k,df_imputed.where(col(k).isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42634951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85620c80",
   "metadata": {},
   "source": [
    "iii.\t\tConvert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore.\n",
    "\t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8093d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+-----+-----+----------------+-------+-------+--------+-----------+---------+--------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|SibSp|Parch|          Ticket|   Fare|  Cabin|Embarked|Age_Imputed|Sex_index|Embarked_index|\n",
      "+-----------+--------+------+--------------------+------+-----+-----+----------------+-------+-------+--------+-----------+---------+--------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|    1|    0|       A/5 21171|   7.25|Missing|       S|       22.0|      0.0|           0.0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|    1|    0|        PC 17599|71.2833|    C85|       C|       38.0|      1.0|           1.0|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|    0|    0|STON/O2. 3101282|  7.925|Missing|       S|       26.0|      1.0|           0.0|\n",
      "+-----------+--------+------+--------------------+------+-----+-----+----------------+-------+-------+--------+-----------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers=[StringIndexer(inputCol=column,outputCol=column+'_index').fit(df_imputed) for column in ['Sex','Embarked']]\n",
    "pipeline=Pipeline(stages=indexers)\n",
    "df_imputed=pipeline.fit(df_imputed).transform(df_imputed)\n",
    "\n",
    "df_imputed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4683b6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = false)\n",
      " |-- Embarked: string (nullable = false)\n",
      " |-- Age_Imputed: double (nullable = true)\n",
      " |-- Sex_index: double (nullable = false)\n",
      " |-- Embarked_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imputed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d7d7e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Age_Imputed: double (nullable = true)\n",
      " |-- Sex_index: double (nullable = false)\n",
      " |-- Embarked_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final=df_imputed.drop('PassengerId','Name','Ticket','Cabin','Sex','Embarked')\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f97136a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#titanic_df = df_imputed.withColumn(\"Age\",df_imputed.Age.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e5280",
   "metadata": {},
   "source": [
    "iv.\t\tUsing vectorAssembler combines all columns (except target column i.e., 'Survived’) of spark DataFrame into single column (name as features).   DataFrame should now contains only two columns features and Survived.\t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9129083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+-------+-----------+---------+--------------+--------------------+\n",
      "|Survived|Pclass|SibSp|Parch|   Fare|Age_Imputed|Sex_index|Embarked_index|            features|\n",
      "+--------+------+-----+-----+-------+-----------+---------+--------------+--------------------+\n",
      "|       0|     3|    1|    0|   7.25|       22.0|      0.0|           0.0|[3.0,1.0,0.0,7.25...|\n",
      "|       1|     1|    1|    0|71.2833|       38.0|      1.0|           1.0|[1.0,1.0,0.0,71.2...|\n",
      "+--------+------+-----+-----+-------+-----------+---------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature=VectorAssembler(inputCols=df_final.columns[1:],outputCol='features')\n",
    "feature_vector=feature.transform(df_final)\n",
    "feature_vector.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea6dc7",
   "metadata": {},
   "source": [
    "v.\t\tSplit the vectorized DataFrame into training and test sets with one third records being held for testing \t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cc6f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData) = feature_vector.randomSplit([.67,0.33],seed=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adda13",
   "metadata": {},
   "source": [
    "vi.\t\tTrain default LogisticRegression model with features as 'featuresCol'   and ‘Survived’ as label.\t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69cf2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr=LogisticRegression(labelCol='Survived',featuresCol=\"features\")\n",
    "model=lr.fit(trainData)\n",
    "pred=model.transform(testData)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8105aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|(7,[0,4],[1.0,40.0])|\n",
      "|       0.0|       0|(7,[0,3,4],[1.0,5...|\n",
      "|       0.0|       0|(7,[0,3,4],[1.0,2...|\n",
      "|       0.0|       0|(7,[0,3,4],[1.0,2...|\n",
      "|       0.0|       0|(7,[0,3,4],[1.0,2...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select('prediction','Survived','features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999e407",
   "metadata": {},
   "source": [
    "vii.\t\tFind F1-score of trained models on test set.\t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9ac587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol='Survived',predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64abe954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7921146953405018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_accuracy= evaluator.evaluate(pred)\n",
    "print('Accuracy :',lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f835a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789272030651341"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol='Survived',predictionCol='prediction',metricName='weightedPrecision')\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f131408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7921146953405018"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol='Survived',predictionCol='prediction',metricName='weightedRecall')\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a9b7a",
   "metadata": {},
   "source": [
    "## ODI Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 11:24:32 WARN Utils: Your hostname, ikom-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp4s0)\n",
      "22/03/02 11:24:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ikom/nehal/pyspark/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/02 11:24:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d9053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f757e659ee0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---------+---+---+---+---+---+\n",
      "| Ponting  R T|  Australia|1995-2012|230|165| 51| 14|124|\n",
      "+-------------+-----------+---------+---+---+---+---+---+\n",
      "| Fleming  S P|New Zealand|1994-2007|218| 98|106| 14|105|\n",
      "| Ranatunga  A|  Sri Lanka|1982-1999|193| 89| 95|  9|102|\n",
      "|  Dhoni  M S*|      India|    2004-|186|103| 68| 15| 88|\n",
      "|  Border  A R|  Australia|1979-1994|178|107| 67|  4| 86|\n",
      "|Azharuddin  M|      India|1985-2000|174| 89| 77|  8| 96|\n",
      "+-------------+-----------+---------+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('ODIData.csv',header=True,inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192cef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522c585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ponting  R T,Australia,1995-2012,230,165,51,14,124',\n",
       " 'Fleming  S P,New Zealand,1994-2007,218,98,106,14,105',\n",
       " 'Ranatunga  A,Sri Lanka,1982-1999,193,89,95,9,102',\n",
       " 'Dhoni  M S*,India,2004-,186,103,68,15,88',\n",
       " 'Border  A R,Australia,1979-1994,178,107,67,4,86']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odiData = sc.textFile( \"ODIData.csv\")\n",
    "type(odiData)\n",
    "odiData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d218b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = (\"name\", \"country\", \"career\", \"matches\", \"won\", \"lost\", \"ties\", \"toss\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05645c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Captains'>\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Captain=namedtuple('Captains',fields)\n",
    "print(Captain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseRecs( line ):\n",
    "  fields = line.split(\",\")\n",
    "  return Captain( fields[0], fields[1], fields[2], int( fields[3] ),\n",
    "                 int( fields[4] ), int(fields[5]), int(fields[6]), int(fields[7] ) )\n",
    "\n",
    "captains = odiData.map( lambda rec: parseRecs( rec) )\n",
    "\n",
    "captains.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1352ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Captains(name='Ponting  R T', country='Australia', career='1995-2012', matches=230, won=165, lost=51, ties=14, toss=124),\n",
       " Captains(name='Fleming  S P', country='New Zealand', career='1994-2007', matches=218, won=98, lost=106, ties=14, toss=105),\n",
       " Captains(name='Ranatunga  A', country='Sri Lanka', career='1982-1999', matches=193, won=89, lost=95, ties=9, toss=102),\n",
       " Captains(name='Dhoni  M S*', country='India', career='2004-', matches=186, won=103, lost=68, ties=15, toss=88),\n",
       " Captains(name='Border  A R', country='Australia', career='1979-1994', matches=178, won=107, lost=67, ties=4, toss=86)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captains.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7475be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captains.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e9c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captains.filter(lambda rec : rec.matches>100).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703cf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Captains(name='Ponting  R T', country='Australia', career='1995-2012', matches=230, won=165, lost=51, ties=14, toss=124),\n",
       " Captains(name='Border  A R', country='Australia', career='1979-1994', matches=178, won=107, lost=67, ties=4, toss=86),\n",
       " Captains(name='Waugh  S R', country='Australia', career='1986-2002', matches=106, won=67, lost=35, ties=4, toss=48),\n",
       " Captains(name='Clarke  M J', country='Australia', career='2003-2015', matches=74, won=50, lost=21, ties=3, toss=30),\n",
       " Captains(name='Taylor  M A', country='Australia', career='1989-1997', matches=67, won=36, lost=30, ties=1, toss=34)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captains.filter(lambda rec : rec.country == 'Australia').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Captains(name='Ponting  R T', country='Australia', career='1995-2012', matches=230, won=165, lost=51, ties=14, toss=124),\n",
       " Captains(name='Dhoni  M S*', country='India', career='2004-', matches=186, won=103, lost=68, ties=15, toss=88),\n",
       " Captains(name='Border  A R', country='Australia', career='1979-1994', matches=178, won=107, lost=67, ties=4, toss=86),\n",
       " Captains(name='Azharuddin  M', country='India', career='1985-2000', matches=174, won=89, lost=77, ties=8, toss=96),\n",
       " Captains(name='Smith  G C', country='South Africa', career='2002-2013', matches=149, won=91, lost=51, ties=7, toss=74)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wins_more_than_losses=captains.filter(lambda rec: rec.won > rec.lost )\n",
    "wins_more_than_losses.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3b07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ponting  R T', 'Dhoni  M S*', 'Border  A R', 'Azharuddin  M', 'Smith  G C']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wins_more_than_losses.map(lambda rec: rec.name).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7de2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Australia', 230),\n",
       " ('New Zealand', 218),\n",
       " ('Sri Lanka', 193),\n",
       " ('India', 186),\n",
       " ('Australia', 178)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total matches per country\n",
    "\n",
    "d1=captains.map(lambda rec : (rec.country,rec.matches))\n",
    "d1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Australia', 832),\n",
       " ('India', 770),\n",
       " ('South Africa', 463),\n",
       " ('Pakistan', 781),\n",
       " ('West Indies', 658)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol=d1.reduceByKey(lambda a,b : a+b)\n",
    "sol.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d47ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Afghanistan', 50),\n",
       " ('Australia', 832),\n",
       " ('Bangladesh', 251),\n",
       " ('Bermuda', 31),\n",
       " ('Canada', 27)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.sortByKey().collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425644a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Canada', 27),\n",
       " ('Netherlands', 31),\n",
       " ('Bermuda', 31),\n",
       " ('Afghanistan', 50),\n",
       " ('Ireland', 93),\n",
       " ('Kenya', 114),\n",
       " ('Bangladesh', 251),\n",
       " ('Zimbabwe', 394),\n",
       " ('South Africa', 463),\n",
       " ('England', 554),\n",
       " ('New Zealand', 608),\n",
       " ('West Indies', 658),\n",
       " ('Sri Lanka', 710),\n",
       " ('India', 770),\n",
       " ('Pakistan', 781),\n",
       " ('Australia', 832)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.sortBy(lambda rec : rec[1]).collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b1329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Australia', 832),\n",
       " ('Pakistan', 781),\n",
       " ('India', 770),\n",
       " ('Sri Lanka', 710),\n",
       " ('West Indies', 658)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.sortBy(lambda rec : rec[1],ascending=False).collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d142e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Captains(name='Ponting  R T', country='Australia', career='1995-2012', matches=230, won=165, lost=51, ties=14, toss=124),\n",
       " Captains(name='Fleming  S P', country='New Zealand', career='1994-2007', matches=218, won=98, lost=106, ties=14, toss=105),\n",
       " Captains(name='Ranatunga  A', country='Sri Lanka', career='1982-1999', matches=193, won=89, lost=95, ties=9, toss=102),\n",
       " Captains(name='Dhoni  M S*', country='India', career='2004-', matches=186, won=103, lost=68, ties=15, toss=88),\n",
       " Captains(name='Border  A R', country='Australia', career='1979-1994', matches=178, won=107, lost=67, ties=4, toss=86)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captains.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8eb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ponting  R T', 53.91304347826087),\n",
       " ('Fleming  S P', 48.1651376146789),\n",
       " ('Ranatunga  A', 52.84974093264248),\n",
       " ('Dhoni  M S*', 47.31182795698925),\n",
       " ('Border  A R', 48.31460674157304)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luck=captains.map(lambda rec : (rec.name , (rec.toss/rec.matches)*100))\n",
    "luck.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443d27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Adams  J C', 69.23076923076923),\n",
       " ('Ramiz Raja', 68.18181818181817),\n",
       " ('Wright  J G', 67.74193548387096),\n",
       " ('Mohammad Ashraful', 65.78947368421053),\n",
       " ('Chigumbura  E*', 64.28571428571429)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luck.sortBy(lambda rec : rec[1],ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09253b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
